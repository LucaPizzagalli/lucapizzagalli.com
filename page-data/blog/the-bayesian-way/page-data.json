{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-url-js","path":"/blog/the-bayesian-way/","result":{"data":{"markdownRemark":{"html":"<h1>The Bayesian Way</h1>\n<p>Ok, we cannot trust the <a href=\"/blog/maximum-likelihood-is-a-lie\">maximum likelihood</a>. So what's next, are we lost?\nNope, we are not. There is a light coming to save us, the light it's called <em>bayesian inference</em>.\nYou are a big boy, you already know the bayes theorem, no need for a recap.</p>\n<details>\n<summary>Actually, maybe...</summary><div>\n<p>The idea of bayesian inference is that if you have some theories about a fact of the universe, but you are not sure which theory is the correct one, you can represent your uncertainty by assigning a probability to each theory. If we call one of your theories <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(A)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span></span></span></span></span> will be how sure you are that <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span> is the correct theory. We call <em>prior</em> the distribution of these probabilities.</p>\n<p>Then an experiment is done, and you get new data <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span>. Using the <em>Bayes theorem</em> you can find how confident you should be now of a theory, given the new information (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(A|X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span></span>). The updated probability distribution is called <em>posterior</em>.</p>\n<p>Here our friend Bayes theorem:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mi mathvariant=\"normal\">∣</mi><mi>A</mi><mo stretchy=\"false\">)</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mi>o</mi><mi>f</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mi mathvariant=\"normal\">∣</mi><mi>K</mi><mo stretchy=\"false\">)</mo><mi>a</mi><mi>l</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>i</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi><mi>A</mi><mo separator=\"true\">,</mo><mi>B</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>D</mi><mo separator=\"true\">,</mo><mi>E</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">P(A|X) = \\frac{P(X|A) P(A)}{sum of P(X|K) along all possible theories A,B,C,D,E}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">llp</span><span class=\"mord mathnormal\">oss</span><span class=\"mord mathnormal\">ib</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">eor</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">es</span><span class=\"mord mathnormal\">A</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></div>\n</details></div>\n<p>How do we apply Bayes to our example?</p>\n<ol>\n<li>We start with a prior describing the probability that the number of cards is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">N=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span>, or that is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">N=2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">N=3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">3</span></span></span></span></span>...</li>\n<li>Using the Bayes theorem we update it with the information that we extracted card number 14.</li>\n<li>We'll have the posterior, telling us what's now our best prediction for <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">N=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span>.</li>\n</ol>\n<p>Easy! Done! No problem at all! Child's game! Let's go home.</p>\n<hr>\n<p>What are you saying? I hear some grumbling over there.\n<em>\"How am I suppose to choose a prior?\"</em>, <em>\"Picking one, over the infinite amount of possible priors, means deciding a priori that some values of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> are more probable than others. But I am supposed to know nothing about the deck, therefore that violates the problem.\"</em>, <em>\"The result will depend strongly on my prior, especially in this settings, where I have only one data point.\"</em>, <em>\"You know what would spare me from picking a prior? The maximu</em></p>\n<p>You sneaky sneaky bastard. I thought we had moved past that. But I'll fix you, I promise I'll convince you to drop the beast forever.\nBut we have to make a little de-tour for that. Close your eyes, grab my hand, take a deep breath and I'll bring you to a new beautiful place.</p>\n<h2>The place</h2>\n<p>You open your eyes, you are in a busy street</p>\n<p>In front of you there are two boxes. Inside one of them there is an unknown amount of candies, in the other one, twice as many. You don't know which box is which.</p>\n<p>The magician explain the rules to you:</p>\n<p>You pick the box you want and you open it. After counting the candies, you can decide to bring them home with you, <strong>or</strong>, you can switch box and bring home the candies in the other box. Only one little thing: for the ability of switching box, you have to pay a modest fee of 2% of the candies you'll find in the second box (even powerful magician have to pay the rent).</p>\n<p>What do you do? (Your goal is to maximize the expected number of candies)</p>\n<hr>\n<p><em>\"What kind of stupid riddle is this? Do you think I'm a fool?\"</em> I know what you are thinking <em>\"I have no idea which box contains the most candies, and even after I open the first box, I still have no idea which box contains the most candies. So why should I pay a fee for switching box? If whathever number I see in the first box, I always switch for the second one, then why not picking the second one directly, and avoid the fee? Keeping the box is obviously the best choice. Not even mr Maximum Likelihood would be stupid enough to switch box (well, maybe).\"</em></p>\n<p>And you are right.</p>\n<p>He's right. You think again about it, he's actually right. But you were right too. You are both right.\nThis cannot be, it's a paradox.</p>\n<p>You destroyed it</p>\n<hr>\n<h2>Test space</h2>\n<p>BLa bla bla bla\nBLa bla bla bla\nBLa bla bla bla</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fbd10a18ca2743ff81ca66d464151bba/c5bb3/newplot.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACsklEQVQoz22TTUiUQRjH95YHCwrM1/1IpExRt919v3bXiuhbY9tdrUj8ykwTy6AMKw9RRlZQFBWSFJFFh+hSdKlDFHUpIugSRSm4+76I35eELu76i5lcQ2jggec/M89v/s8M4xgZGSGZTGJZlgzbthfyjP7fnGXbJBIJfs3MkAZSqRRzc3M4YrE4paWl6LqOqqqEQqGFXIRpmjIyWtM0QuGw3FNUXMzTJ08gnWJsYprZVApHNBqlpKREbvT7/QSDQVkYCASkNgxDhsjFnFgLmkF0w2BDcy3PXjyHySnGJqelS0e8qgqv1ytdCGh5ebkECAdCiwOEa03X0A0dwzQJGQZbqqPsvtzNq88fEEO4E8MRi8UWWhYOZLGm/Ws5GJSH+X0+Aj4fmnRoEunqYNelM7z88kmCUun0/B3G45SVlUlXAhAOh2Uuof4ApqphqioVrY1UtDSgetexo34/tdfOE+s+xvt3b/4CM48SF0CvF104CgQkUPP5JUxfH8bcsY3tbU1U3e5l7/UeKnpO0dHawA1d5XK+h6+PH0tgenZ2vuVIhNK1RQQ3b0LN3NHO7VQeb6NaFLcf5NDZTrqqI7S1N9F6+hh3CtdwLyeHviVZfBsYWAyM1NcRbqqh/mI38c529jTXcbDrKDU3LtC7eSO3lDz6Cwp4sHwFdz0e+t1uuvKc9K5aRV92Nh8fPvzbcgZ47mQHpyq3cqFwDT0lRdwvLqLf5eJK4WpO5ORQryjcVBSuKQpHVq6kJTeXGrebRqeTzqws3s47XABeNQwOZy+lzumiMTeXs3lOTrhcNCgK+91uWXzA7aHd5WLfvK51e6jNzye6bBmvHz1aDByxbUZtmwnbZsyyGPoxyGgyybhlMSW+2M9BrMEhxm2LKctibDjB8PcfTFoW44kEv2dmJEi8sAD+Adr6QYqocsxcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Alt text here\"\n        title=\"some title\"\n        src=\"/static/fbd10a18ca2743ff81ca66d464151bba/c5bb3/newplot.png\"\n        srcset=\"/static/fbd10a18ca2743ff81ca66d464151bba/0b533/newplot.png 500w,\n/static/fbd10a18ca2743ff81ca66d464151bba/c5bb3/newplot.png 680w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">some title</figcaption>\n  </figure></p>\n<p>BLa bla bla bla\nBLa bla bla bla\nBLa bla bla bla\nBLa bla bla bla</p>","frontmatter":{"date":"July 28, 2022","url":"/blog/the-bayesian-way","title":"The Bayesian Way"}}},"pageContext":{"id":"a8cae3e9-c701-5954-bd60-bc9924d05450","frontmatter__url":"/blog/the-bayesian-way","__params":{"frontmatter__url":"blog"}}},"staticQueryHashes":["63159454"]}